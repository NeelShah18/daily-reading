# entropy

interpretation: expected number of bits under *optimal encoding*

optimal encoding: allocation of bits to different "events". 

- the more frequent the events, the fewer bits, need `\log{\frac{1}{p_i}}` for event `i` with proba `p_i`



# cross entropy

expected number of bits using noisy distribution

always larger than entropy (accurate distribution)

also known as the training objective in probabilistic prediction / machine learning


# KL divergence

difference between cross entropy and entropy


# resource

- https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/