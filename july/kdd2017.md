# Finding Structures of Large Matrices Through Compression


learned:

- goal of paper: **scalable** low-rank approximation
- what low-rank approximation do
  - still a on-going topic
- application of low-rank: IR, RecSys, model compression (deep learning), etc
- prev state of the art based on randomized algorithm
  - memory consumption as bottle neck because it requires the whole matrix as input
- the paper goal: tackle the bottleneck using lossy data compression
  - achieves **linear** time and space


# Scalable and Sustainable Deep Learning via Randomized Hashing

- using locality sensitive hashing in training deep neural network

learned:

- dropout:
  - main idea: *randomly drop* units during *training* to prevent overfitting (a lot of parameters *co-adapting*)
- adaptive dropout: selectively drop units based on each unit's activation (gradient value?)
  - core question: identify the *sparse* set of neurons *efficiently*? 
  - this is what this paper studies (using locality sensitive hashing (LSH))
- LSH: maps similar points to the same hash value (bucket) with high probability
  - how to define "similarity" among neurons?
  - what's the hashing function?

- foreign language translation (google translate for image): also uses deep learning, maybe also easier than i thought (no image segmantation?)


# TrioVecEvent: Embedding-Based Online Local Event Detection in Geo-Tagged Tweet Streams

- geo-tagged "posts" (tweets, facebook, instagram, yelp, foursquare, etc)
  - location, time and text
- local event defined as: unsual busrty activity in a local region within short time and with a considerable amount of participants
- main idea: embed location, time and text into the same latent space
  - applicable in streaming case
  - claim to be efficient and accurate 

# Learning certifiably optimal rule lists for categorical data

- interesting point: interpretable and less interpretable classifiers with the same accuracy, how to find such interpretable classifiers?
- build decision rule list over *categorical* feature space
  - optimal and fast
- Q: how to define optimality?

# Clustering Individual Transactional Data for Masses of Users

- input: search engine queries, purchase records or clinical records
- why it's useful?
- what are the features? how to define similarity?

# Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks

- [paper](http://www.cse.ust.hk/~qyaoaa/papers/kdd2017paper.pdf)
- collaborative filtering: matrix completion can be solved by low rank approximation
  - problem 1: cold start (no data for new users)
  - problem 2: sparsity
- meta-graph: seems to be dervied from meta-path (multiple meta graphs)
- *Q*: how is recommendation done exactly?
- meta graph selection using *group lasso*


# Effective and Real-time In-App Activity Analysis in Encrypted Internet Traffic Streams

- problem: classify user activity into different types from **encrypted** app-activity traffic
  - requirement: low memory and high accuracy

# FORA: Simple and Effective Approximate Single-Source Personalized PageRank

- problem: given a query, return the top-k results w.r.t personalized pagerank
- goal: do it efficiently

# A Minimal Variance Estimator for the Cardinality of Big Data Set Intersection

- problem: estimate size of intersection of set A and B
- idea: store **sketch** of the set and use **probabilistic estimator**
- application: 
  - optimizing join queries in dataset (A join B join C)
     - which two tables to join? (a join query simply performs set intersection)
  - document similarity based on set intersection
- further reading: sketch and estimation

# Online Ranking with Constraints: A Primal-Dual Algorithm and Applications to Web Traffic-Shaping

- online ranking with local objective (user engagement) and global constraint (num. of clicks/views)
- algorihtm based on linear programming duality 
- with **theoretical gaurantees**

# 

