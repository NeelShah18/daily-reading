# title


## Contribution

- A generalization of convolution neural network from 2D euclidian space to graphs (non-euclidean).
- Recursive formulaiton for fast filtering
- 

## Questions

- why convert it to "frequency domain"?
- what does it mean to be graph frequency domain?

## Learned:


- Hadamard product: element-wise product
- Kronecker delta: `detal_{ij} = 1` if `i=j` otherwise 0. Related to eye function and vector product.
- Spectral domain: aka frequency domain
  - a signal can be seen as a mixture of "component signals" of different frequencies.
  - however, each frequency component has a certain strength/weight.
  - a function that transforms the signal between time and frequency is called an "transform".
  - an example transform is Fourier transform
  - [nice visualization from Wikipedia](https://en.wikipedia.org/wiki/Frequency_domain)
- Convolution theorem
  - example: convolution in one domain (e.g, time) equals point-wise multiplication in the other domain
  - [more reading](https://en.wikipedia.org/wiki/Convolution_theorem)
- Spectral Graph Theory (concepts)
  - node signal `x`: a vector
  - graph laplacian matrix `L=U \Lambda U^T`
  - orthonormal eigen vectors in `U` of L known as **graph Fourier modes**
  - `\Lambda` eigenvalues known as **frequencies of the graph**
  - graph fourier transform: `U^T x`
  - maybe the UC Berkley video

- K-localized
- Learning complexity
- Chebyshev expansion




Application of node embedding:

- visualization
- node/edge classification


CNN feature: learn local stationary features and compose them into multi-scale hierarchical patterns.

# Background on CNN

## CNN

- "filter": a small 2D/3D matrix, matrix entries are the filter parameters. The depth of the filter should be the same as the image. For image with 3 color channels, the depth is 3.
  - can be seen as shape detector (curve, circle, line, segments, etc)
- "convolve" operation: dot product between a image block and the filter parameter
  - if the resulting value is large, it means the receptive field has such shape associated by the filter. 
- receptive field: the image block that a filter applies upon
- convolution layer: apply k filters to the image. Each filter generates a new "image" of depth one (called activative/feature map). So together, it generates k feature maps (or 2D image of depth k)
- stride: how fast the filter moves. If stride is k, it moves k pixels a time.

## Background: spectral graph theory

[Video](https://www.youtube.com/watch?v=01AqmIU9Su4)

- another property of eigenvalues as "minimization problem"
- using graph laplacian as the matrix and the above property leads to minimum/maximum cut problem.


## Pooling



Visualization methods of CNN filters

- "guided" backpropagation
- gradient ascent: backprop onto the original image
